{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "\n",
    "=================================================================\n",
    "\n",
    "## Mean Normalisation\n",
    "\n",
    "\n",
    "Mean normalisation involves centering the variable at zero, and re-scaling to the value range. The procedure involves subtracting the mean of each observation and then dividing by difference between the minimum and maximum value:\n",
    "\n",
    "**x_scaled = (x - x_mean) / ( x_max - x_min)**\n",
    "\n",
    "\n",
    "The result of the above transformation is a distribution that is centered at 0, and its minimum and maximum values are within the range of -1 to 1. The shape of a mean normalised distribution will be very similar to the original distribution of the variable, but the variance may change, so not identical.\n",
    "\n",
    "Again, this technique will not **normalize the distribution of the data** thus if this is the desired outcome, we should implement any of the techniques discussed in section 7 of the course.\n",
    "\n",
    "In a nutshell, mean normalisation:\n",
    "\n",
    "- centers the mean at 0\n",
    "- variance will be different\n",
    "- may alter the shape of the original distribution\n",
    "- the minimum and maximum values squeezed between -1 and 1\n",
    "- preserves outliers\n",
    "\n",
    "Good for algorithms that require features centered at zero.\n",
    "\n",
    "\n",
    "## In this demo\n",
    "\n",
    "We will perform mean normalisation using the Boston House Prices data set that comes with Scikit-learn\n",
    "\n",
    "There is no Scikit-learn transformer for mean normalisation, but we can implement it using a combination of 2 other transformers. We will also implement it manually with pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# dataset for the demo\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# the scaler - for mean normalisation\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the the Boston House price data\n",
    "\n",
    "# this is how we load the boston dataset from sklearn\n",
    "boston_dataset = load_boston()\n",
    "\n",
    "# create a dataframe with the independent variables\n",
    "data = pd.DataFrame(boston_dataset.data,\n",
    "                      columns=boston_dataset.feature_names)\n",
    "\n",
    "# add target\n",
    "data['MEDV'] = boston_dataset.target\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information about the boston house prince dataset\n",
    "# you will find details about the different variables\n",
    "\n",
    "# the aim is to predict the \"Median value of the houses\"\n",
    "# MEDV column in this dataset\n",
    "\n",
    "# and there are variables with characteristics about\n",
    "# the homes and the neighborhoods\n",
    "\n",
    "# print the dataset description\n",
    "print(boston_dataset.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's have a look at the main statistical parameters of the variables\n",
    "# to get an idea of the feature magnitudes\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different variables present different value ranges, mean, max, min, standard deviations, etc. In other words, they show different magnitudes or scales. Note for this demo, how **the mean values are not centered at zero, and the min and max value vary across a big range**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing mean normalisation on the data set, we need to first identify the mean and minimum and maximum values of the variables. These parameters need to be learned from the train set, stored, and then used to scale test and future data. Thus, we will first divide the data set into train and test, as we have done throughout the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's separate the data into training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('MEDV', axis=1),\n",
    "                                                    data['MEDV'],\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Normalisation with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first learn the mean from the train set\n",
    "\n",
    "means = X_train.mean(axis=0)\n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's now learn the min and max values, and the value range \n",
    "# from the train set\n",
    "\n",
    "ranges = X_train.max(axis=0)-X_train.min(axis=0)\n",
    "ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we are ready to perform mean normalisation:\n",
    "\n",
    "X_train_scaled = (X_train - means) / ranges\n",
    "X_test_scaled = (X_test - means) / ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's have a look at the original training dataset: mean and min, max values\n",
    "# I use np.round to reduce the number of decimals to 1.\n",
    "\n",
    "np.round(X_train.describe(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's have a look at the scaled training dataset:  mean and min, max values\n",
    "# I use np.round to reduce the number of decimals to 1.\n",
    "\n",
    "np.round(X_train_scaled.describe(), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the mean of each variable, which were not centered at zero, is now around zero and the min and max values vary approximately between -1 and 1. Note however, that the standard deviations vary according to how spread the variable was to begin with and is highly influenced by the presence of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's compare the variable distributions before and after scaling\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n",
    "\n",
    "# before scaling\n",
    "ax1.set_title('Before Scaling')\n",
    "sns.kdeplot(X_train['RM'], ax=ax1)\n",
    "sns.kdeplot(X_train['LSTAT'], ax=ax1)\n",
    "sns.kdeplot(X_train['CRIM'], ax=ax1)\n",
    "\n",
    "# after scaling\n",
    "ax2.set_title('After Mean Normalisation')\n",
    "sns.kdeplot(X_train_scaled['RM'], ax=ax2)\n",
    "sns.kdeplot(X_train_scaled['LSTAT'], ax=ax2)\n",
    "sns.kdeplot(X_train_scaled['CRIM'], ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the main effect of mean normalisation was to center all the distributions at zero, and the values vary between -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's compare the variable distributions before and after scaling\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n",
    "\n",
    "# before scaling\n",
    "ax1.set_title('Before Scaling')\n",
    "sns.kdeplot(X_train['AGE'], ax=ax1)\n",
    "sns.kdeplot(X_train['DIS'], ax=ax1)\n",
    "sns.kdeplot(X_train['NOX'], ax=ax1)\n",
    "\n",
    "# after scaling\n",
    "ax2.set_title('After Mean Normalisation')\n",
    "sns.kdeplot(X_train_scaled['AGE'], ax=ax2)\n",
    "sns.kdeplot(X_train_scaled['DIS'], ax=ax2)\n",
    "sns.kdeplot(X_train_scaled['NOX'], ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare these plots, with those derived by standardisation in the previous notebook to better understand how these procedures are not identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Mean Normalisation with Scikit-learn: work-around\n",
    "\n",
    "We can implement mean normalisation by combining the use of 2 transformers. A bit dirty, if you ask me, but if you are desperate to implement this technique with sklearn, this could be a way forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the StandardScaler so that it removes the mean\n",
    "# but does not divide by the standard deviation\n",
    "scaler_mean = StandardScaler(with_mean=True, with_std=False)\n",
    "\n",
    "# set up the robustscaler so that it does NOT remove the median\n",
    "# but normalises by max()-min(), important for this to set up the\n",
    "# quantile range to 0 and 100, which represent the min and max values\n",
    "scaler_minmax = RobustScaler(with_centering=False,\n",
    "                             with_scaling=True,\n",
    "                             quantile_range=(0, 100))\n",
    "\n",
    "# fit the scalers to the train set, it will learn the parameters\n",
    "scaler_mean.fit(X_train)\n",
    "scaler_minmax.fit(X_train)\n",
    "\n",
    "# transform train and test sets\n",
    "X_train_scaled = scaler_minmax.transform(scaler_mean.transform(X_train))\n",
    "X_test_scaled = scaler_minmax.transform(scaler_mean.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's transform the returned NumPy arrays to dataframes for the rest of\n",
    "# the demo\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(X_train_scaled.describe(), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how this output is identical to that of cell 10, where we did the scaling manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's have a look at the scaled training dataset:  mean and min, max values\n",
    "# I use np.round to reduce the number of decimals to 1.\n",
    "\n",
    "np.round(X_train_scaled.describe(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
